{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Description\n",
        "\n",
        "##Preprocessed data taken from a study done by Nanyang Technological University Singapore\n",
        "\n",
        "https://arxiv.org/abs/2106.00613\n",
        "\n"
      ],
      "metadata": {
        "id": "n1BhYTkEBWGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset from Google Drive"
      ],
      "metadata": {
        "id": "zIeuz9nlW1Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from keras import layers, Model"
      ],
      "metadata": {
        "id": "u0nfoU2LoRkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQZoum1hWA6O",
        "outputId": "b6ec3263-5d66-4307-abb2-514450cfc9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the .mat file on your Google Drive\n",
        "mat_file_path = '/content/drive/MyDrive/Capstone/dataset.mat'\n",
        "\n",
        "# Load the .mat file\n",
        "mat_data = loadmat(mat_file_path)\n"
      ],
      "metadata": {
        "id": "biwUOIfoXETP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Transformer Model"
      ],
      "metadata": {
        "id": "aImjGrNJaOme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((seq_length, d_model))\n",
        "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
        "    return tf.convert_to_tensor(pos_enc[..., np.newaxis], dtype=tf.float32)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, seq_length, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = get_positional_encoding(seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding\n",
        "\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_model, dff, name=\"feedforward\"):\n",
        "        super(FeedForward, self).__init__(name=name)\n",
        "        self.dense1 = layers.Dense(dff, activation='relu')\n",
        "        self.dense2 = layers.Dense(d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, name=\"encoder_layer\"):\n",
        "        super(EncoderLayer, self).__init__(name=name)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.mha(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "        return output\n",
        "\n",
        "class EEGTransformer(Model):\n",
        "    def __init__(self, seq_length, d_model, num_heads, num_layers, num_classes, dff, dropout_rate, num_channels):\n",
        "        super().__init__()\n",
        "        self.conv1d = layers.Conv1D(d_model, kernel_size=10, padding='same', activation='relu')\n",
        "        self.positional_encoding = PositionalEncoding(seq_length, d_model)\n",
        "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "        self.classification_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.positional_encoding(inputs)\n",
        "        x = self.conv1d(x)\n",
        "        for i in range(len(self.encoder_layers)):\n",
        "            x = self.encoder_layers[i](x, training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.global_average_pooling(x)\n",
        "        output = self.classification_layer(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "LMyF6rbIZfjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split and Train the Data"
      ],
      "metadata": {
        "id": "CDJajAHGaRa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EEGsample = mat_data['EEGsample']\n",
        "subindex = mat_data['subindex']\n",
        "substate = mat_data['substate']\n",
        "\n",
        "# Initialize a DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['Subject', 'Channel', 'Average Accuracy', 'Average Loss', 'Average F1 Score', 'Average AUC Score'])\n",
        "\n",
        "# Define the number of repeats and the number of folds\n",
        "n_repeats = 20\n",
        "n_splits = 5\n",
        "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "\n",
        "# Iterate through all patient IDs\n",
        "for patient_id in range(1, 12):  # Assuming there are 11 patients with IDs from 1 to 11\n",
        "    print(f\"Processing subject {patient_id}\")\n",
        "\n",
        "    # Extract data for the current patient\n",
        "    patient_mask = subindex == patient_id\n",
        "    X = EEGsample[np.squeeze(patient_mask)]\n",
        "    y = substate[np.squeeze(patient_mask)]\n",
        "\n",
        "    model = EEGTransformer(\n",
        "        seq_length=384,\n",
        "        d_model=30, # Adjust this according to the number of channels or desired dimensionality\n",
        "        num_heads=8,\n",
        "        num_layers=4,\n",
        "        num_classes=2,\n",
        "        dff=128,\n",
        "        dropout_rate=0.3,\n",
        "        num_channels=30\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00006), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "    y_tensor = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "    # Initialize lists to store the fold accuracies, losses, F1 scores, and AUC scores\n",
        "    fold_accuracies = []\n",
        "    fold_losses = []\n",
        "    fold_f1_scores = []\n",
        "    fold_auc_scores = []\n",
        "\n",
        "    # Iterate through the repeats of the k-fold cross-validation\n",
        "    for train_index, test_index in rskf.split(X_tensor.numpy(), y_tensor.numpy()):\n",
        "        # Create the training and test sets\n",
        "        X_train, X_test = X_tensor.numpy()[train_index], X_tensor.numpy()[test_index]\n",
        "        y_train, y_test = y_tensor.numpy()[train_index], y_tensor.numpy()[test_index]\n",
        "\n",
        "        # Further split the training data into training (80%) and validation (20%) sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "        # Convert the numpy arrays back into tensors\n",
        "        X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "        X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "        y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
        "        X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "        y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(train_dataset, validation_data=val_dataset, epochs=30, verbose=0)\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "        # Compute predictions for F1 and AUC scores for test data\n",
        "        y_pred_raw = model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_raw, axis=-1)\n",
        "\n",
        "        # Compute F1 scores and AUC scores for test data\n",
        "        test_f1_score = f1_score(y_test, y_pred, average='weighted')\n",
        "        test_auc_score = roc_auc_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # Append the accuracies and losses to their respective lists\n",
        "        fold_accuracies.append(test_accuracy)\n",
        "        fold_losses.append(test_loss)\n",
        "        fold_f1_scores.append(test_f1_score)\n",
        "        fold_auc_scores.append(test_auc_score)\n",
        "\n",
        "    # Compute the average accuracies and losses\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    avg_loss = np.mean(fold_losses)\n",
        "    avg_f1_score = np.mean(fold_f1_scores)\n",
        "    avg_auc_score = np.mean(fold_auc_scores)\n",
        "\n",
        "    # Add the results to the DataFrame (outside the channel loop)\n",
        "    results_df = results_df.append({\n",
        "        'Subject': patient_id,\n",
        "        'Average Accuracy': avg_accuracy,\n",
        "        'Average Loss': avg_loss,\n",
        "        'Average F1 Score': avg_f1_score,\n",
        "        'Average AUC Score': avg_auc_score\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(results_df)\n",
        "results_df.to_excel('results.xlsx', index=False)\n",
        "# Download the Excel file\n",
        "from google.colab import files\n",
        "files.download('results.xlsx')\n"
      ],
      "metadata": {
        "id": "Ysw1wU5QpEVQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30f36d14-f421-4756-bb0b-391c0a4f3257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8bfbdea7a576>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'eeg_transformer_1/positional_encoding_1/add' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-7-8bfbdea7a576>\", line 67, in <cell line: 14>\n      history = model.fit(train_dataset, validation_data=val_dataset, epochs=30, verbose=0)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"<ipython-input-6-ae8830db295e>\", line 59, in call\n      x = self.positional_encoding(inputs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"<ipython-input-6-ae8830db295e>\", line 15, in call\n      return inputs + self.pos_encoding\nNode: 'eeg_transformer_1/positional_encoding_1/add'\nrequired broadcastable shapes\n\t [[{{node eeg_transformer_1/positional_encoding_1/add}}]] [Op:__inference_train_function_4944]"
          ]
        }
      ]
    }
  ]
}