{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1BhYTkEBWGI"
      },
      "source": [
        "#Dataset Description\n",
        "\n",
        "##Preprocessed data taken from a study done by Nanyang Technological University Singapore\n",
        "\n",
        "https://arxiv.org/abs/2106.00613\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIeuz9nlW1Tl"
      },
      "source": [
        "# Load Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQZoum1hWA6O",
        "outputId": "88080bcf-fae5-42ff-bcae-dec7a7581d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biwUOIfoXETP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "# Path to the .mat file on your Google Drive\n",
        "mat_file_path = '/content/drive/MyDrive/Capstone/dataset.mat'\n",
        "\n",
        "# Load the .mat file\n",
        "mat_data = loadmat(mat_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHZopuMm3CJ"
      },
      "outputs": [],
      "source": [
        "EEGsample = mat_data['EEGsample']\n",
        "subindex = mat_data['subindex']\n",
        "substate = mat_data['substate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImjGrNJaOme"
      },
      "source": [
        "# Build the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMyF6rbIZfjX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, Model\n",
        "\n",
        "def get_positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((seq_length, d_model))\n",
        "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return tf.convert_to_tensor(pos_enc[..., np.newaxis], dtype=tf.float32)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, seq_length, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = get_positional_encoding(seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      return inputs + tf.squeeze(self.pos_encoding, axis=2)\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, d_model, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (-1, x.shape[1], self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear layers\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Split heads\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # Scale dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Masking\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        # Dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Context vector\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        # Concatenate heads\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_model, dff, name=\"feedforward\"):\n",
        "        super(FeedForward, self).__init__(name=name)\n",
        "        self.dense1 = layers.Dense(dff, activation='relu')\n",
        "        self.dense2 = layers.Dense(d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, name=\"encoder_layer\"):\n",
        "        super(EncoderLayer, self).__init__(name=name)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.mha(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "        return output\n",
        "\n",
        "class EEGTransformer(Model):\n",
        "    def __init__(self, seq_length, d_model, num_heads, num_layers, num_classes, dff, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.conv1d = layers.Conv1D(d_model, kernel_size=25, padding='same', activation='relu')\n",
        "        self.positional_encoding = PositionalEncoding(seq_length, d_model)\n",
        "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "        self.classification_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.positional_encoding(inputs)\n",
        "        x = self.conv1d(x)\n",
        "        for i in range(len(self.encoder_layers)):\n",
        "            x = self.encoder_layers[i](x, training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.global_average_pooling(x)\n",
        "        output = self.classification_layer(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDJajAHGaRa8"
      },
      "source": [
        "# Split and Train the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovEn4UCx4T_-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysw1wU5QpEVQ",
        "outputId": "6160194c-52fe-4bc9-c52d-291cfc12c3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7a952cb9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7a95e63a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 1 results:\n",
            "Train Accuracy: 0.7022, Validation Accuracy: 0.6866, Test Accuracy: 0.6649\n",
            "Train Loss: 0.6022, Validation Loss: 0.6032, Test Loss: 0.6456\n",
            "F1 Score: 0.6560, ROC-AUC: 0.7008\n",
            "\n",
            "\n",
            "Processing subject 2\n",
            "Subject 2 results:\n",
            "Train Accuracy: 0.7167, Validation Accuracy: 0.7180, Test Accuracy: 0.5182\n",
            "Train Loss: 0.5954, Validation Loss: 0.5852, Test Loss: 0.7223\n",
            "F1 Score: 0.6231, ROC-AUC: 0.6062\n",
            "\n",
            "\n",
            "Processing subject 3\n",
            "Subject 3 results:\n",
            "Train Accuracy: 0.7092, Validation Accuracy: 0.6821, Test Accuracy: 0.6307\n",
            "Train Loss: 0.6082, Validation Loss: 0.6168, Test Loss: 0.6518\n",
            "F1 Score: 0.6051, ROC-AUC: 0.6738\n",
            "\n",
            "\n",
            "Processing subject 4\n",
            "Subject 4 results:\n",
            "Train Accuracy: 0.7169, Validation Accuracy: 0.7029, Test Accuracy: 0.6122\n",
            "Train Loss: 0.5934, Validation Loss: 0.5911, Test Loss: 0.6663\n",
            "F1 Score: 0.6485, ROC-AUC: 0.6533\n",
            "\n",
            "\n",
            "Processing subject 5\n",
            "Subject 5 results:\n",
            "Train Accuracy: 0.6990, Validation Accuracy: 0.6828, Test Accuracy: 0.6920\n",
            "Train Loss: 0.6073, Validation Loss: 0.6176, Test Loss: 0.6104\n",
            "F1 Score: 0.6369, ROC-AUC: 0.7579\n",
            "\n",
            "\n",
            "Processing subject 6\n",
            "Subject 6 results:\n",
            "Train Accuracy: 0.6987, Validation Accuracy: 0.7011, Test Accuracy: 0.7373\n",
            "Train Loss: 0.6274, Validation Loss: 0.6252, Test Loss: 0.6124\n",
            "F1 Score: 0.7472, ROC-AUC: 0.8100\n",
            "\n",
            "\n",
            "Processing subject 7\n",
            "Subject 7 results:\n",
            "Train Accuracy: 0.7074, Validation Accuracy: 0.6901, Test Accuracy: 0.5471\n",
            "Train Loss: 0.6066, Validation Loss: 0.6196, Test Loss: 0.7034\n",
            "F1 Score: 0.6231, ROC-AUC: 0.6115\n",
            "\n",
            "\n",
            "Processing subject 8\n",
            "Subject 8 results:\n",
            "Train Accuracy: 0.6973, Validation Accuracy: 0.6778, Test Accuracy: 0.7159\n",
            "Train Loss: 0.6159, Validation Loss: 0.6169, Test Loss: 0.6002\n",
            "F1 Score: 0.7378, ROC-AUC: 0.7936\n",
            "\n",
            "\n",
            "Processing subject 9\n",
            "Subject 9 results:\n",
            "Train Accuracy: 0.6807, Validation Accuracy: 0.6637, Test Accuracy: 0.6771\n",
            "Train Loss: 0.6211, Validation Loss: 0.6267, Test Loss: 0.6264\n",
            "F1 Score: 0.5868, ROC-AUC: 0.7512\n",
            "\n",
            "\n",
            "Processing subject 10\n",
            "Subject 10 results:\n",
            "Train Accuracy: 0.7005, Validation Accuracy: 0.7003, Test Accuracy: 0.7500\n",
            "Train Loss: 0.6147, Validation Loss: 0.6136, Test Loss: 0.5725\n",
            "F1 Score: 0.7242, ROC-AUC: 0.8002\n",
            "\n",
            "\n",
            "Processing subject 11\n",
            "Subject 11 results:\n",
            "Train Accuracy: 0.7093, Validation Accuracy: 0.7172, Test Accuracy: 0.5619\n",
            "Train Loss: 0.6074, Validation Loss: 0.6022, Test Loss: 0.7091\n",
            "F1 Score: 0.6748, ROC-AUC: 0.6516\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "iter = 5\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Iterate through all patient IDs\n",
        "for patient_id in range(1, 12):  # Assuming there are 11 patients with IDs from 1 to 11\n",
        "    print(f\"Processing subject {patient_id}\")\n",
        "\n",
        "    # Combine data from all subjects except the test_patient_id\n",
        "    train_patient_mask = subindex != patient_id\n",
        "    test_patient_mask = subindex == patient_id\n",
        "    X_train_all = EEGsample[np.squeeze(train_patient_mask)]\n",
        "    y_train_all = substate[np.squeeze(train_patient_mask)]\n",
        "\n",
        "    channel_indices = [1, 2, 4, 5, 6, 14, 15, 16, 24, 29]\n",
        "    X_selected_channels_train = X_train_all[:, channel_indices, :]\n",
        "    X_train_selected = np.transpose(X_selected_channels_train, (0, 2, 1))\n",
        "\n",
        "    X_test_all = EEGsample[np.squeeze(test_patient_mask)]\n",
        "    y_test_all = substate[np.squeeze(test_patient_mask)]\n",
        "\n",
        "    channel_indices = [1, 2, 4, 5, 6, 14, 15, 16, 24, 29]\n",
        "    X_selected_channels_test = X_test_all[:, channel_indices, :]\n",
        "    X_test_selected = np.transpose(X_selected_channels_test, (0, 2, 1))\n",
        "    # print(f\"Size of test subject: {X_test_all.shape} \")\n",
        "    # print(f\"Size of every other subject: {X_train_all.shape} \")\n",
        "\n",
        "\n",
        "    # Initialize an empty dictionary to store the test accuracies for each channel\n",
        "    channel_accuracies = {}\n",
        "    # Initialize an empty dictionary to store the losses for each channel\n",
        "    channel_losses = {}\n",
        "\n",
        "\n",
        "    # Initialize lists to store the train, validation, and test accuracies for the current channel\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    test_accuracies = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    f1_scores = []\n",
        "    roc_aucs = []\n",
        "\n",
        "\n",
        "    for i in range(iter):\n",
        "\n",
        "        model = EEGTransformer(\n",
        "                seq_length=384,\n",
        "                d_model=10,\n",
        "                num_heads=4,\n",
        "                num_layers=2,\n",
        "                num_classes=2,\n",
        "                dff=64,\n",
        "                dropout_rate=0.3\n",
        "            )\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=Adam(learning_rate=0.000005), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        # Assuming x_normalized contains the preprocessed (normalized) epochs\n",
        "        X_train_tensor = tf.convert_to_tensor(X_train_selected, dtype=tf.float32)\n",
        "        X_test_tensor = tf.convert_to_tensor(X_test_selected, dtype=tf.float32)\n",
        "\n",
        "        # Assuming y contains the sleep stage labels\n",
        "        y_train_tensor = tf.convert_to_tensor(y_train_all, dtype=tf.int32)\n",
        "        y_test_tensor = tf.convert_to_tensor(y_test_all, dtype=tf.int32)\n",
        "\n",
        "        # Split the training data into training (80%) and validation (20%) sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_tensor.numpy(), y_train_tensor.numpy(), test_size=0.2)\n",
        "\n",
        "        X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "        X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "        y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test_tensor))\n",
        "\n",
        "\n",
        "        batch_size = 64\n",
        "        buffer_size = len(X_train)\n",
        "\n",
        "        train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        # Train the model\n",
        "        epochs = 100\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=epochs,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Get the training and validation accuracies and losses from the history object\n",
        "        train_accuracy = history.history['accuracy'][-1]\n",
        "        val_accuracy = history.history['val_accuracy'][-1]\n",
        "        train_loss = history.history['loss'][-1]\n",
        "        val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "        #Get the F1 score\n",
        "        # Generate predictions on the test dataset\n",
        "        y_pred = model.predict(test_dataset, verbose = 0)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "        # Flatten the tensors to 1D arrays for plotting\n",
        "        y_test_array = y_test_tensor.numpy().flatten()\n",
        "        y_pred_array = y_pred_classes.flatten()\n",
        "        f1 = f1_score(y_test_array, y_pred_array)\n",
        "\n",
        "        # Compute the predicted probabilities of the positive class\n",
        "        y_pred_probs = y_pred[:, 1]\n",
        "\n",
        "        # Compute the AUC-ROC\n",
        "        roc_auc = roc_auc_score(y_test_array, y_pred_probs)\n",
        "\n",
        "        # Append the accuracies and losses to their respective lists\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        f1_scores.append(f1)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "\n",
        "    # Compute the average accuracies and losses for the current channel\n",
        "    avg_train_accuracy = np.mean(train_accuracies)\n",
        "    avg_val_accuracy = np.mean(val_accuracies)\n",
        "    avg_test_accuracy = np.mean(test_accuracies)\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    avg_test_loss = np.mean(test_losses)\n",
        "    avg_f1_score = np.mean(f1_scores)\n",
        "    avg_roc_auc = np.mean(roc_aucs)\n",
        "\n",
        "    # After calculating your averages, append them to the all_results list\n",
        "    all_results.append({\n",
        "        'Subject ID': patient_id,\n",
        "        'Train Accuracy': avg_train_accuracy,\n",
        "        'Validation Accuracy': avg_val_accuracy,\n",
        "        'Test Accuracy': avg_test_accuracy,\n",
        "        'Train Loss': avg_train_loss,\n",
        "        'Validation Loss': avg_val_loss,\n",
        "        'Test Loss': avg_test_loss,\n",
        "        'F1 Score': avg_f1_score,\n",
        "        'ROC-AUC': avg_roc_auc\n",
        "    })\n",
        "\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Subject {patient_id} results:\")\n",
        "    print(f\"Train Accuracy: {avg_train_accuracy:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}, Test Accuracy: {avg_test_accuracy:.4f}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"F1 Score: {avg_f1_score:.4f}, ROC-AUC: {avg_roc_auc:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# After your patient loop, convert the list of results to a DataFrame\n",
        "df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel(\"results.xlsx\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}