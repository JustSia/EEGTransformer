{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1BhYTkEBWGI"
      },
      "source": [
        "#Dataset Description\n",
        "\n",
        "##Preprocessed data taken from a study done by Nanyang Technological University Singapore\n",
        "\n",
        "https://arxiv.org/abs/2106.00613\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIeuz9nlW1Tl"
      },
      "source": [
        "# Load Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQZoum1hWA6O",
        "outputId": "a61c1d7a-2d4a-48fb-e381-2dc5cba6df7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biwUOIfoXETP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "# Path to the .mat file on your Google Drive\n",
        "mat_file_path = '/content/drive/MyDrive/Capstone/dataset.mat'\n",
        "\n",
        "# Load the .mat file\n",
        "mat_data = loadmat(mat_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHZopuMm3CJ"
      },
      "outputs": [],
      "source": [
        "patient_id = 11\n",
        "EEGsample = mat_data['EEGsample']\n",
        "subindex = mat_data['subindex']\n",
        "substate = mat_data['substate']\n",
        "\n",
        "patient_mask = subindex == patient_id\n",
        "X = EEGsample[np.squeeze(patient_mask)]\n",
        "y = substate[np.squeeze(patient_mask)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBW_gUBlXmjQ"
      },
      "outputs": [],
      "source": [
        "# X = mat_data['EEGsample']\n",
        "# y = mat_data['substate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6DJxZrtYQU6",
        "outputId": "f7a8d30b-cb59-4821-d3c2-d5b55be52d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(226, 30, 384)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_8vB9MZY9MH"
      },
      "outputs": [],
      "source": [
        "oz_index = 28\n",
        "X_oz = X[:, oz_index, :]\n",
        "\n",
        "X_oz = np.expand_dims(X_oz, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4g69vFvaqXR",
        "outputId": "39b8b6ca-a2b4-45ca-9ee4-75983e174de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(226, 384, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_oz.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImjGrNJaOme"
      },
      "source": [
        "# Build the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMyF6rbIZfjX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, Model\n",
        "\n",
        "def get_positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((seq_length, d_model))\n",
        "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return tf.convert_to_tensor(pos_enc[..., np.newaxis], dtype=tf.float32)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, seq_length, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = get_positional_encoding(seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      return inputs + tf.squeeze(self.pos_encoding, axis=2)\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, d_model, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (-1, x.shape[1], self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear layers\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Split heads\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # Scale dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Masking\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        # Dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Context vector\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        # Concatenate heads\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_model, dff, name=\"feedforward\"):\n",
        "        super(FeedForward, self).__init__(name=name)\n",
        "        self.dense1 = layers.Dense(dff, activation='relu')\n",
        "        self.dense2 = layers.Dense(d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, name=\"encoder_layer\"):\n",
        "        super(EncoderLayer, self).__init__(name=name)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.mha(inputs, inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        output = self.layernorm2(out1 + ffn_output)\n",
        "        return output\n",
        "\n",
        "class EEGTransformer(Model):\n",
        "    def __init__(self, seq_length, d_model, num_heads, num_layers, num_classes, dff, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.conv1d = layers.Conv1D(d_model, kernel_size=10, padding='same', activation='relu')\n",
        "        self.positional_encoding = PositionalEncoding(seq_length, d_model)\n",
        "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "        self.classification_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.positional_encoding(inputs)\n",
        "        x = self.conv1d(x)\n",
        "        for i in range(len(self.encoder_layers)):\n",
        "            x = self.encoder_layers[i](x, training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.global_average_pooling(x)\n",
        "        output = self.classification_layer(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvPmWumoZmxK"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "# Instantiate the model\n",
        "model = EEGTransformer(\n",
        "    seq_length=384,\n",
        "    d_model=16,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    num_classes=2,\n",
        "    dff=64,\n",
        "    dropout_rate=0.3\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDJajAHGaRa8"
      },
      "source": [
        "# Split and Train the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovEn4UCx4T_-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysw1wU5QpEVQ",
        "outputId": "aa4d8e1b-d261-41e9-e594-a40fd16c5ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 11 results:\n",
            "Train Accuracy: 0.8840, Validation Accuracy: 0.8167, Test Accuracy: 0.8109\n",
            "Train Loss: 0.2679, Validation Loss: 0.3756, Test Loss: 0.3908\n",
            "F1 Score: 0.8006, ROC-AUC: 0.8677\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "channel_indices = [1, 2, 4, 5, 6, 14, 15, 16, 24, 29]\n",
        "\n",
        "X_selected_channels = X[:, channel_indices, :]\n",
        "\n",
        "iter = 50\n",
        "\n",
        "# Initialize an empty dictionary to store the test accuracies for each channel\n",
        "channel_accuracies = {}\n",
        "# Initialize an empty dictionary to store the losses for each channel\n",
        "channel_losses = {}\n",
        "# Initialize lists to store the train, validation, and test accuracies for the current channel\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "f1_scores = []\n",
        "auc_scores = []\n",
        "\n",
        "\n",
        "for i in range(iter):\n",
        "  model = EEGTransformer(\n",
        "    seq_length=384,\n",
        "    d_model=10,\n",
        "    num_heads=6,\n",
        "    num_layers=3,\n",
        "    num_classes=2,\n",
        "    dff=128,\n",
        "    dropout_rate=0.35\n",
        "  )\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=Adam(learning_rate=0.000008), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "  # Extract the current channel data\n",
        "  X_channel = np.transpose(X_selected_channels, (0, 2, 1))\n",
        "\n",
        "  # Assuming x_normalized contains the preprocessed (normalized) epochs\n",
        "  X_tensor = tf.convert_to_tensor(X_channel, dtype=tf.float32)\n",
        "\n",
        "  # Assuming y contains the sleep stage labels\n",
        "  y_tensor = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "  # Split the data into training (80%) and test (20%) sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_tensor.numpy(), y_tensor.numpy(), test_size=0.2)\n",
        "\n",
        "  # Further split the training data into training (80%) and validation (20%) sets\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "  X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "  y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
        "  X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "  y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "  batch_size = 16\n",
        "  buffer_size = len(X_train)\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # Train the model\n",
        "  epochs = 1000\n",
        "\n",
        "  # Early stopping\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      validation_data=val_dataset,\n",
        "      epochs=epochs,\n",
        "      callbacks=[early_stopping],\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Get the training and validation accuracies and losses from the history object\n",
        "  train_accuracy = history.history['accuracy'][-1]\n",
        "  val_accuracy = history.history['val_accuracy'][-1]\n",
        "  train_loss = history.history['loss'][-1]\n",
        "  val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "  # Compute predictions for F1 and AUC scores for test data\n",
        "  y_test_pred = model.predict(X_test, verbose = 0)\n",
        "\n",
        "  y_pred_probs = y_test_pred[:, 1]\n",
        "\n",
        "  # Convert probabilities to binary outputs\n",
        "  y_test_pred_binary = np.argmax(y_test_pred, axis=-1)\n",
        "\n",
        "  y_test_flatten = y_test.numpy().flatten()\n",
        "  y_test_pred_flatten = y_test_pred_binary.flatten()\n",
        "\n",
        "  # Compute F1 scores and AUC scores for test data\n",
        "  test_f1_score = f1_score(y_test_flatten, y_test_pred_flatten)\n",
        "  test_auc_score = roc_auc_score(y_test_flatten, y_pred_probs)\n",
        "\n",
        "  # Append the F1 scores and AUC scores for test data to their respective lists\n",
        "  f1_scores.append(test_f1_score)\n",
        "  auc_scores.append(test_auc_score)\n",
        "\n",
        "  # Append the accuracies and losses to their respective lists\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  test_accuracies.append(test_accuracy)\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "# Compute the average accuracies and losses for the current channel\n",
        "avg_train_accuracy = np.mean(train_accuracies)\n",
        "avg_val_accuracy = np.mean(val_accuracies)\n",
        "avg_test_accuracy = np.mean(test_accuracies)\n",
        "avg_train_loss = np.mean(train_losses)\n",
        "avg_val_loss = np.mean(val_losses)\n",
        "avg_test_loss = np.mean(test_losses)\n",
        "avg_f1_score = np.mean(f1_scores)\n",
        "avg_auc_score = np.mean(auc_scores)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# After calculating your averages, append them to the all_results list\n",
        "all_results.append({\n",
        "    'Subject ID': patient_id,\n",
        "    'Train Accuracy': avg_train_accuracy,\n",
        "    'Validation Accuracy': avg_val_accuracy,\n",
        "    'Test Accuracy': avg_test_accuracy,\n",
        "    'Train Loss': avg_train_loss,\n",
        "    'Validation Loss': avg_val_loss,\n",
        "    'Test Loss': avg_test_loss,\n",
        "    'F1 Score': avg_f1_score,\n",
        "    'ROC-AUC': avg_auc_score\n",
        "})\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(f\"Subject {patient_id} results:\")\n",
        "print(f\"Train Accuracy: {avg_train_accuracy:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}, Test Accuracy: {avg_test_accuracy:.4f}\")\n",
        "print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"F1 Score: {avg_f1_score:.4f}, ROC-AUC: {avg_auc_score:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# After your patient loop, convert the list of results to a DataFrame\n",
        "df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel(\"results.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}